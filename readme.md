# ğŸ¤– Agentic RAG Chatbot with LangGraph

A production-ready, modular Retrieval-Augmented Generation (RAG) chatbot built with LangGraph, featuring:
- Multi-format document support (PDF, DOCX, TXT, CSV, PPTX, XLSX)
- HuggingFace embeddings for semantic search
- Groq LLM integration for fast inference
- Intelligent web search fallback
- Conversation memory management
- Streamlit web interface

## ğŸ—ï¸ Architecture

The chatbot uses a **LangGraph workflow** with the following nodes:

```
User Query â†’ Retrieve (from docs) â†’ Check Relevance â†’ [Relevant?]
                                                          â†“
                                                    Yes â†’ Compose Answer
                                                          â†“
                                                    No â†’ Web Search â†’ Compose Answer
                                                          â†“
                                                    Update Memory â†’ End
```

## ğŸ“‹ Features

- âœ… **Multi-Format Support**: Upload PDF, DOCX, TXT, CSV, PPTX, XLSX files
- âœ… **Semantic Search**: FAISS vector store with HuggingFace embeddings
- âœ… **Smart Fallback**: Automatic web search when documents lack information
- âœ… **Conversation Memory**: Context-aware responses using LangChain memory
- âœ… **Modular Design**: Clean separation of concerns across modules
- âœ… **Source Attribution**: Clear labeling of information sources
- âœ… **Session Management**: Multiple conversation sessions
- âœ… **Real-time Processing**: Progress indicators and status updates

## ğŸš€ Installation

### 1. Clone the repository

```bash
git clone <repository-url>
cd agentic_rag_chatbot
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up API keys

Get your Groq API key from [https://console.groq.com](https://console.groq.com)

## ğŸ¯ Usage

### Run the Streamlit app

```bash
streamlit run app.py
```

### Using the chatbot

1. **Configure API Key**: Enter your Groq API key in the sidebar
2. **Select Model**: Choose from available Groq models (default: llama-3.1-70b-versatile)
3. **Upload Documents**: Upload one or more files (PDF, DOCX, etc.)
4. **Process Files**: Click "Process Files" to create the vector store
5. **Start Chatting**: Ask questions about your documents!

## ğŸ“ Project Structure

```
agentic_rag_chatbot/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI and main application
â”œâ”€â”€ graph_agent.py         # LangGraph workflow definition
â”œâ”€â”€ data_loader.py         # Multi-format file processing
â”œâ”€â”€ embedding.py           # HuggingFace embeddings manager
â”œâ”€â”€ vectorstore.py         # FAISS vector store operations
â”œâ”€â”€ search.py              # Document search + web search
â”œâ”€â”€ memory_manager.py      # Conversation memory handling
â”œâ”€â”€ utils.py               # Helper functions and utilities
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

## ğŸ”§ Configuration

### Model Options

Available Groq models:
- `llama-3.1-70b-versatile` (default) - Best balance of speed and quality
- `llama-3.1-8b-instant` - Fastest responses
- `mixtral-8x7b-32768` - Large context window
- `gemma2-9b-it` - Efficient and accurate

### Relevance Threshold

Adjust the relevance threshold (0.0 - 1.0) to control when web search is triggered:
- **Higher values (0.7-0.9)**: More strict, triggers web search more often
- **Lower values (0.3-0.5)**: More lenient, uses documents more often

### Embedding Models

Default: `sentence-transformers/all-MiniLM-L6-v2`

Other options:
- `sentence-transformers/all-mpnet-base-v2` (higher quality, slower)
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (multilingual)

## ğŸ§© Module Descriptions

### `data_loader.py`
- Supports multiple file formats
- Text extraction and cleaning
- Chunking with RecursiveCharacterTextSplitter
- Document metadata management

### `embedding.py`
- HuggingFace embeddings integration
- Lazy loading for efficiency
- Normalized embeddings for better similarity

### `vectorstore.py`
- FAISS vector store creation and management
- Similarity search with scores
- Persistent storage support
- Add/delete operations

### `search.py`
- Document search with relevance scoring
- DuckDuckGo web search integration
- Hybrid search strategy
- Context formatting

### `memory_manager.py`
- Conversation buffer memory
- Session management
- Chat history retrieval
- Export capabilities

### `graph_agent.py`
- LangGraph workflow orchestration
- Five-node agent architecture
- Groq LLM integration
- State management

### `app.py`
- Streamlit UI components
- File upload and processing
- Chat interface
- Session state management

## ğŸ“Š How It Works

### 1. Document Processing
```
Upload Files â†’ Extract Text â†’ Clean Text â†’ Chunk Text â†’ Create Embeddings â†’ Store in FAISS
```

### 2. Query Processing
```
User Query â†’ Embed Query â†’ Search Documents â†’ Check Relevance
    â†“
[High Relevance] â†’ Use Document Context â†’ Generate Answer
    â†“
[Low Relevance] â†’ Web Search â†’ Use Web Results â†’ Generate Answer
```

### 3. Memory Management
```
Each Conversation Turn â†’ Save to Memory â†’ Include in Next Query Context
```

## ğŸ” Example Queries

**From Documents:**
- "What are the main points in the uploaded report?"
- "Summarize the findings from the research paper"
- "What does the document say about XYZ?"

**Web Fallback:**
- "What's the latest news about AI?"
- "Who won the recent election?"
- "What's the weather like today?"

**Context-Aware:**
- "Tell me more about that" (references previous conversation)
- "Can you explain it differently?" (uses chat history)

## ğŸ› ï¸ Customization

### Add New File Types

Edit `data_loader.py`:
```python
def load_custom_format(self, file_bytes: bytes, filename: str) -> str:
    # Your custom loader logic
    return extracted_text
```

### Change Vector Store

Edit `vectorstore.py` to use Chroma or other vector stores:
```python
from langchain_community.vectorstores import Chroma
# Replace FAISS with Chroma
```

### Modify Agent Workflow

Edit `graph_agent.py` to add/remove nodes or change logic:
```python
workflow.add_node("custom_node", self.custom_node_function)
workflow.add_edge("check_relevance", "custom_node")
```

## ğŸ“ˆ Performance Tips

1. **Chunk Size**: Adjust `chunk_size` in DataLoader for optimal retrieval
2. **Top-K Results**: Modify `top_k` in search functions for more/fewer results
3. **Model Selection**: Use smaller models for faster responses
4. **Relevance Threshold**: Fine-tune based on your use case

## ğŸ› Troubleshooting

### "No module named X"
```bash
pip install -r requirements.txt
```

### "API Key Error"
- Verify your Groq API key is correct
- Check API key has sufficient credits

### "Vector Store Not Found"
- Ensure files are processed before querying
- Check the vectorstore directory exists

### "Memory Error"
- Reduce chunk size in data_loader.py
- Process fewer files at once

## ğŸ“ License

This project is open-source and available under the MIT License.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ™ Acknowledgments

- LangChain & LangGraph for the framework
- Groq for fast LLM inference
- HuggingFace for embeddings
- Streamlit for the UI framework

## ğŸ“§ Contact

For questions or support, please open an issue on the repository.

---

**Built with â¤ï¸ using LangGraph, Groq, and Streamlit**